{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNSUPERVISED LEARNING INTUITION \n",
    "**LEARNING THE STRUCTURE OR PROBABILITY DISTRIBUTION OF THE DATA**  \n",
    "    o\tDensity Estimation  \n",
    "    o\tLatent Variables  \n",
    "        \tUnderlying cause  \n",
    "        \tMissing or hidden data  \n",
    "        \tYou have a bunch of documents and find distinct clusters  \n",
    "\n",
    "**K-MEANS TRAINING ALGORITHM **     \n",
    "    o\tInitialize m(1),m(2)…m(k) random points in X  \n",
    "    o\tWhile not converged  \n",
    "        \tDecide which cluster each point X belongs to  \n",
    "        \tRecalculate cluster centers based on the X’s that were assigned to it (i.e. mean)  \n",
    "        \tDo this until the algorithm converges (no changes in the cluster assignments or centers)  \n",
    "\n",
    "**PROBLEMS WITH K-MEANS**  \n",
    "        \tHighly sensitive to initialization  \n",
    "        \tPossible resolution: restart multiple times, use whichever result gives us the best final objective  \n",
    "        \tWhat does this tell us? Local minima  \n",
    "        \tAnother possible resolution: ‘fuzzy’ membership in each class  \n",
    "        \tJust a small adjustment to the original k-means algorithm  \n",
    "\n",
    "**DISADVANTAGES OF K-MEANS**  \n",
    "        \tYou have to choose K  \n",
    "        \tLocal minima  \n",
    "        \tSensitive to initial configuration  \n",
    "        \tCan’t solve donut problem  \n",
    "\n",
    "**GAUSSIAN MIXTURE MODELS (GMM)**    \n",
    "    o\tGive us an approximation of the probability distribution of our data  \n",
    "    o\tUse Gaussian Mixture Models when we notice our data is multi-model (multiple modes/ bumps)  \n",
    "        \tMode: most common value  \n",
    "    o\tGaussian mixture is the sum of weighted Gaussians \n",
    "\n",
    "**TRAINING A GMM  **    \n",
    "    o\tSimilar to K-Means: first initialize all parameters to random values  \n",
    "        \tCalculate responsibilities  \n",
    "        \tCalculate model parameters  \n",
    "\n",
    "**COMPARISON BETWEEN GMM AND SOFT K-MEANS**   \n",
    "    o\tK-means looks for clusters of equal weight because it has no PI variable  \n",
    "        \tEquivalent to saying pi is uniform or equal to 1 over k  \n",
    "    o\tCan think of soft K-means as a GMM where each cluster has the same weight and is spherical with the same radius\n",
    "    \n",
    "**KERNEL DENSITY ESTIMATION  **  \n",
    "    o\tFitting of a probability with kerns  \n",
    "    o\tEasiest Density Estimation  \n",
    "        \tHistogram\t  \n",
    "    o\tKernel Density Estimation  \n",
    "    \tJust use a Gaussian mixture model  \n",
    "    \tGaussian = the kernel   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
